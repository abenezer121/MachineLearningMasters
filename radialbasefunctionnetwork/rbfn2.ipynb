{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fLength' 'fWidth' 'fSize' 'fConc' 'fConc1' 'fAsym' 'fM3Long' 'fM3Trans'\n",
      " 'fAlpha' 'fDist' 'class']\n"
     ]
    }
   ],
   "source": [
    "# read the dataset\n",
    "trainingdata = pd.read_csv('./dataset/magic04.csv')\n",
    "trainingdata['class'] = np.where(trainingdata['class'] == \"g\", 1, 0)\n",
    "labels = trainingdata.columns.values\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# separate the column and the row\n",
    "def scale_dataset(dataframe, oversample=False):\n",
    "    X = dataframe[dataframe.columns[:-1]].values\n",
    "    Y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "    scalar = StandardScaler()\n",
    "    # take x and fit the scalar transform all the values\n",
    "    X = scalar.fit_transform(X)\n",
    "    # if one dataset is way much higher than the other one this will make the model to prefer the data with higher number of class\n",
    "    # so we make the data equal\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler()\n",
    "        X, Y = ros.fit_resample(X, Y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The target 'y' needs to have more than 1 class. Got 1 class instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      2\u001b[0m \u001b[39m# scale and normalize the data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m xtrain, ytrain \u001b[39m=\u001b[39m scale_dataset(trainingdata[:\u001b[39m5000\u001b[39;49m], oversample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m, in \u001b[0;36mscale_dataset\u001b[1;34m(dataframe, oversample)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mif\u001b[39;00m oversample:\n\u001b[0;32m     14\u001b[0m     ros \u001b[39m=\u001b[39m RandomOverSampler()\n\u001b[1;32m---> 15\u001b[0m     X, Y \u001b[39m=\u001b[39m ros\u001b[39m.\u001b[39;49mfit_resample(X, Y)\n\u001b[0;32m     17\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\base.py:84\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     81\u001b[0m arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m     82\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m     85\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling_strategy, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampling_type\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     88\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m     90\u001b[0m y_ \u001b[39m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m     label_binarize(output[\u001b[39m1\u001b[39m], classes\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39munique(y)) \u001b[39mif\u001b[39;00m binarize_y \u001b[39melse\u001b[39;00m output[\u001b[39m1\u001b[39m]\n\u001b[0;32m     92\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\imblearn\\utils\\_validation.py:514\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    509\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39msampling_type\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should be one of \u001b[39m\u001b[39m{\u001b[39;00mSAMPLING_KIND\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msampling_type\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m     )\n\u001b[0;32m    513\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39munique(y)\u001b[39m.\u001b[39msize \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 514\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    515\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe target \u001b[39m\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m\u001b[39m needs to have more than 1 class. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39munique(y)\u001b[39m.\u001b[39msize\u001b[39m}\u001b[39;00m\u001b[39m class instead\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m sampling_type \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mensemble\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbypass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    520\u001b[0m     \u001b[39mreturn\u001b[39;00m sampling_strategy\n",
      "\u001b[1;31mValueError\u001b[0m: The target 'y' needs to have more than 1 class. Got 1 class instead"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# scale and normalize the data\n",
    "xtrain, ytrain = scale_dataset(trainingdata, oversample=True)\n",
    "Xtrain,  Ytrain, Xtest, Ytest = train_test_split(xtrain, ytrain,  random_state=104,\n",
    "                                                 test_size=0.25,\n",
    "                                                 shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNN:\n",
    "\n",
    "    def __init__(self, kernels, centers, beta=0.1, lr=0.1, epochs=80) -> None:\n",
    "\n",
    "        self.kernels = kernels\n",
    "        self.centers = centers\n",
    "        self.beta = beta\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.b = np.random.randn(1, 1)\n",
    "        self.W = np.random.randn(kernels, 1)\n",
    "       \n",
    "\n",
    "      \n",
    "\n",
    "        # to save the gradients calculated by the network\n",
    "   \n",
    "        self.gradients = []\n",
    "\n",
    "    def rbf_activation(self, x, center):\n",
    "        return np.exp(-self.beta*np.linalg.norm(x - center)**2)\n",
    "\n",
    "    def linear_activation(self, A):\n",
    "        return self.W.T.dot(A) + self.b\n",
    "\n",
    "    def least_square_error(self, pred, y):\n",
    "        return (y - pred).flatten()**2\n",
    "\n",
    "    def _forward_propagation(self, x):\n",
    "\n",
    "        a1 = np.array([\n",
    "            [self.rbf_activation(x, center)]\n",
    "            for center in self.centers\n",
    "        ])\n",
    "\n",
    "        a2 = self.linear_activation(a1)\n",
    "\n",
    "        return a2, a1\n",
    "\n",
    "    def _backpropagation(self, y, pred, a1):\n",
    "        # Back propagation\n",
    "        dW = -(y - pred).flatten()*a1\n",
    "        db = -(y - pred).flatten()\n",
    "\n",
    "        # Updating the weights\n",
    "        self.W = self.W - self.lr*dW\n",
    "        self.b = self.b - self.lr*db\n",
    "        return dW, db\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "\n",
    "            for x, y in list(zip(X, Y)):\n",
    "                # Forward propagation\n",
    "                pred, a1 = self._forward_propagation(x)\n",
    "\n",
    "                error = self.least_square_error(pred[0], y[0, np.newaxis])\n",
    "                \n",
    "\n",
    "                # Back propagation\n",
    "                dW, db = self._backpropagation(y, pred, a1)\n",
    "                self.gradients.append((dW, db))\n",
    "\n",
    "    def predict(self, x):\n",
    "        a2, a1 = self._forward_propagation(x)\n",
    "        \n",
    "        return np.squeeze(a2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting\n",
      " accuracy 56.56827765163801 \n"
     ]
    }
   ],
   "source": [
    "Xtest = Xtest.reshape(len(Xtest), 1)\n",
    "Ytest = Ytest.reshape(len(Ytest), 1)\n",
    "\n",
    "rbf = RBFNN(kernels=2,\n",
    "                centers=np.array([\n",
    "                    [0, 1, 0, 1, 0, 1, 0, 1,1,0],\n",
    "                    [1, 0, 1, 0, 1, 0, 1, 1, 0,1]\n",
    "\n",
    "                ]),\n",
    "                beta=0.31,\n",
    "                lr=0.01,\n",
    "                epochs=100\n",
    "                )\n",
    "\n",
    "rbf.fit(Xtrain, Xtest)\n",
    "\n",
    "\n",
    "num_correct = 0\n",
    "\n",
    "print(\"start predicting\")\n",
    "\n",
    "for i in range(len(Ytrain)):\n",
    "    pred = rbf.predict(Ytrain[i])\n",
    "    p = 1 if pred > 0.5 else 0\n",
    "    \n",
    "    if p == Ytest[i]:\n",
    "         num_correct += 1\n",
    "   \n",
    "print(f' accuracy {(num_correct / len(Ytest)) * 100 } ')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1af9bccfc8dc720bdb4344246e512a802194bbd2352e86f7c49a1ad9815e25fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
